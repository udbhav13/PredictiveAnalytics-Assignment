---
title: "Question5"
author: "Neha Anna John"
date: "18/08/2019"
output: html_document
---
### Declaring libraries
```{r}
library(tm) 
library(magrittr)
library(slam)
library(proxy)
library(caret)
```

### Declaring training and testing data set and getting data from each folder within training and test folders. Also, changing them the naming convention for representation. Post this, training and testing corpus is created
```{r}
readerPlain = function(fname){
				readPlain(elem=list(content=readLines(fname)), 
							id=fname, language='en') }

## Training and Test dataset
setwd("C:/Users/nehaa/Desktop/Predictive Modelling/GitFolder/STA380/data")
trainlist = Sys.glob('ReutersC50/C50train/*/*.txt')
train = lapply(trainlist, readerPlain) 

testlist = Sys.glob('ReutersC50/C50test/*/*.txt')
test = lapply(testlist, readerPlain)

mynames = trainlist %>%
	{ strsplit(., '/', fixed=TRUE) } %>%
	{ lapply(., tail, n=2) } %>%
	{ lapply(., paste0, collapse = '') } %>%
	unlist

mynames_test = testlist %>%
	{ strsplit(., '/', fixed=TRUE) } %>%
	{ lapply(., tail, n=2) } %>%
	{ lapply(., paste0, collapse = '') } %>%
	unlist

names(train) = mynames
names(test) = mynames

authors_train = trainlist %>%
{ strsplit(., '/', fixed=TRUE) } %>%
{ lapply(., function(x) x[3]) } %>%
{ lapply(., paste0, collapse = '') } %>%
  unlist

authors_test = testlist %>%
{ strsplit(., '/', fixed=TRUE) } %>%
{ lapply(., function(x) x[3]) } %>%
{ lapply(., paste0, collapse = '') } %>%
  unlist

train_raw = Corpus(VectorSource(train))
test_raw = Corpus(VectorSource(test))
```


```{r}
my_documents = train_raw
my_documents = tm_map(my_documents, content_transformer(tolower))
my_documents = tm_map(my_documents, content_transformer(removeNumbers)) 
my_documents = tm_map(my_documents, content_transformer(removePunctuation)) 
my_documents = tm_map(my_documents, content_transformer(stripWhitespace)) 

my_documents_test = test_raw
my_documents_test = tm_map(my_documents_test, content_transformer(tolower))
my_documents_test = tm_map(my_documents_test, content_transformer(removeNumbers)) 
my_documents_test = tm_map(my_documents_test, content_transformer(removePunctuation)) 
my_documents_test = tm_map(my_documents_test, content_transformer(stripWhitespace))
```


```{r}
stopwords("en")
my_documents = tm_map(my_documents, content_transformer(removeWords), stopwords("en"))
my_documents_test = tm_map(my_documents_test, content_transformer(removeWords), stopwords("en"))
```

```{r}
train_matrix = DocumentTermMatrix(my_documents)
train_matrix = removeSparseTerms(train_matrix, 0.98)
test_matrix = DocumentTermMatrix(my_documents_test, control = list(dictionary=Terms(train_matrix)))

train_matrix
```

```{r}
tfidf_train = weightTfIdf(train_matrix)
tfidf_test = weightTfIdf(test_matrix)
```

PCA on tfidf
```{r}
X_train = as.matrix(tfidf_train)
X_test = as.matrix(tfidf_test)

scrub_cols = which(colSums(X_train) == 0)
scrub_cols_test = which(colSums(X_test) == 0)

X_train = X_train[,-scrub_cols]
X_test = X_test[,-scrub_cols_test]

pca_train = prcomp(X_train, scale=TRUE)
pca_test=predict(pca_train,newdata = X_test)
plot(pca_train, type='line') 

names(pca_train)
```



```{r}
plot(cumsum(pca_train$sdev^2/sum(pca_train$sdev^2)))
df_train_trunc = data.frame(pca_train$x[,1:700])
df_train_trunc['author'] = authors_train
df_test_trunc = data.frame(pca_test[,1:700])
df_test_trunc['author']=authors_test

```

```{r}
library('randomForest')
rf_model = randomForest(as.factor(author) ~ ., data=df_train_trunc, ntree=500, mtry=8, importance=TRUE)
rf_predictions = predict(rf_model,df_test_trunc)
cm = caret::confusionMatrix(rf_predictions,as.factor(df_test_trunc$author))
accuracy = cm$overall['Accuracy']
accuracy
```


```{r}
library('e1071')
nb_model = naiveBayes(as.factor(author) ~., data=df_train_trunc)
nb_predictions = predict(nb_model,df_test_trunc)
cm = caret::confusionMatrix(nb_predictions,as.factor(df_test_trunc$author))
accuracy <- cm$overall['Accuracy']
accuracy

```








